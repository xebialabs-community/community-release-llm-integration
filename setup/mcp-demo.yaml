---
apiVersion: xl-release/v1
kind: Templates
metadata:
  home: AI Demo
spec:

  # MCP Servers
  - name: Agility MCP Server
    type: community-llm.McpServer
    url: https://api.staging.digital.ai/agilitymcp/v1/mcp
    headers:
      X-Agility-Bearer: !value AGILITY_KEY
      X-Agility-Host: https://www7.v1host.com/V1Production

  - name: Release MCP Server
    type: community-llm.McpServer
    url: http://host.docker.internal:8000/mcp

  - name: GitHub MCP Server
    type: community-llm.McpServer
    url: https://api.githubcopilot.com/mcp/
    headers:
      Authorization: !value GITHUB_TOKEN

    # AI Models
  - name: Gemini 2.5 Pro
    type: community-llm.GeminiModel
    apiKey: !value GEMINI_API_KEY
    model_id: gemini-2.5-pro

  - name: Gemini 2.5 Flash
    type: community-llm.GeminiModel
    apiKey: !value GEMINI_API_KEY
    model_id: gemini-2.5-flash

  - name: Gemini 2.5 Flash-Lite
    type: community-llm.GeminiModel
    apiKey: !value GEMINI_API_KEY
    model_id: gemini-2.5-flash-lite

  - name: Amazon Nova Micro 1:0
    type: community-llm.DaiLlmModel
    url: https://api.staging.digital.ai/llm
    apiKey: !value DAI_LLM_API_KEY
    model_id: amazon.nova-micro-v1:0

  - name: Anthropic Sonnet 4.5
    type: community-llm.DaiLlmModel
    url: https://api.staging.digital.ai/llm
    apiKey: !value DAI_LLM_API_KEY
    model_id: anthropic.claude-sonnet-4-5-20250929-v1:0

  - name: OpenAI GPT 5
    type: community-llm.OpenAiModel
    apiKey: !value OPENAI_API_KEY
    model_id: gpt-5

  - name: OpenAI GPT 5 mini
    type: community-llm.OpenAiModel
    apiKey: !value OPENAI_API_KEY
    model_id: gpt-5-mini

  - name: OpenAI GPT 5 nano
    type: community-llm.OpenAiModel
    apiKey: !value OPENAI_API_KEY
    model_id: gpt-5-nano

  - name: SmolLM2 (local)
    type: community-llm.OpenAiModel
    url: http://model-runner.docker.internal/engines/v1
    apiKey: none
    model_id: smollm2

  - name: Llama 3.2 (local)
    type: community-llm.OpenAiModel
    url: http://model-runner.docker.internal/engines/v1
    apiKey: none
    model_id: llama3.2

  # Demo Templates
  - template: 1. MCP examples
    description: |
      Demo template to show different uses of the **MCP: Call tool** task.
    phases:
      - phase: Call tool
        tasks:
          - name: "Release: List folders"
            type: community-llm.McpCallTool
            server: Release MCP Server
            tool: list_folders
            input: |-
              {
                "request": {}
              }
          - name: "GitHub: Get my details"
            type: community-llm.McpCallTool
            server: GitHub MCP Server
            tool: get_me
          - name: "GitHub: Read sample issue"
            type: community-llm.McpCallTool
            server: GitHub MCP Server
            tool: issue_read
            input: |-
              {
              "issue_number": 1,
              "method": "get",
              "owner": "xebialabs-community",
              "repo": "community-release-llm-integration"
              }
            result: ${issue}
        color: "#3d6c9e"
    variables:
      - type: xlrelease.StringVariable
        key: issue
        requiresValue: false
        showOnReleaseStart: false
        label: Issue
        description: The issue json from GitHub
  - template: 2. Prompt examples
    description: |
      Test different models with the same prompt.
    phases:
      - phase: Test different models
        tasks:
          - name: Gemini
            type: community-llm.LlmPrompt
            taskFailureHandlerEnabled: true
            taskRecoverOp: SKIP_TASK
            prompt: "${prompt}"
            model: Gemini 2.5 Flash-Lite
          - name: OpenAI
            type: community-llm.LlmPrompt
            taskFailureHandlerEnabled: true
            taskRecoverOp: SKIP_TASK
            prompt: "${prompt}"
            model: OpenAI GPT 5 nano
          - name: Nova Micro  (Digital.ai LLM Service)
            type: community-llm.LlmPrompt
            taskFailureHandlerEnabled: true
            taskRecoverOp: SKIP_TASK
            prompt: "${prompt}"
            model: Amazon Nova Micro 1:0
          - name: SmolLM2 (local Docker)
            type: community-llm.LlmPrompt
            taskFailureHandlerEnabled: true
            taskRecoverOp: SKIP_TASK
            prompt: "${prompt}"
            model: Smollm2 (local)
          - name: Llama 3.2 (local Docker)
            type: community-llm.LlmPrompt
            taskFailureHandlerEnabled: true
            taskRecoverOp: SKIP_TASK
            prompt: "${prompt}"
            model: Llama 3.2 (local)
    variables:
      - type: xlrelease.StringVariable
        key: prompt
        label: Prompt
        description: The prompt to be answered by the AI model
        value: |
          Say hello world in six languages, one for each continent
        multiline: true
  - template: 3. MCP + Prompt examples
    description: |
      Shows how to combine the **MCP: Call tool** task with the **AI: prompt** task for further processing.
    phases:
      - phase: User details
        tasks:
          - name: "GitHub: Get my details"
            type: community-llm.McpCallTool
            server: GitHub MCP Server
            tool: get_me
            result: ${user}
          - name: Get name
            type: community-llm.LlmPrompt
            prompt: |-
              Extract the name from the following data. Give me only the full name as a result
              
              ```
              ${user}
              ```
            model: Gemini 2.5 Flash
            response: ${name}
          - name: Get summary
            type: community-llm.LlmPrompt
            prompt: |-
              Please create a nice summary of the following data
              
              ```
              ${user}
              ```
            model: Gemini 2.5 Flash
            response: ${summary}
          - name: "Verify summary for ${name}"
            type: xlrelease.GateTask
            description: "${summary}"
            owner: admin
      - phase: List failed releases
        tasks:
          - name: Find failed releases
            type: community-llm.McpCallTool
            server: Release MCP Server
            tool: list_releases
            input: |-
              {
                "request": 
                {
                 "failed": true
                }
              }
            result: ${releases}
          - name: Create a summary
            type: community-llm.LlmPrompt
            prompt: |-
              Make a summary of the failed releases:
              
              ```
              ${releases}
              ```
            model: Gemini 2.5 Flash
            response: ${release_summary}
          - name: Verify
            type: xlrelease.GateTask
            description: |-
              ## Failed releases
              
              ${release_summary}
            owner: admin
        color: "#3d6c9e"
    variables:
      - type: xlrelease.StringVariable
        key: user
        requiresValue: false
        showOnReleaseStart: false
        label: User info
        description: JSON info of user
      - type: xlrelease.StringVariable
        key: summary
        requiresValue: false
        showOnReleaseStart: false
        label: User summary
        description: The response from Google Gemini
      - type: xlrelease.StringVariable
        key: name
        requiresValue: false
        showOnReleaseStart: false
        label: User name
        description: Subject of the message.
      - type: xlrelease.StringVariable
        key: releases
        requiresValue: false
        showOnReleaseStart: false
        label: ßReleases
        description: Result
      - type: xlrelease.StringVariable
        key: release_summary
        requiresValue: false
        showOnReleaseStart: false
        label: Failed release summary
  - template: 4. Agent examples
    description: |
      Example usage of the **AI: Agent** task
    phases:
      - phase: Agent with MCP
        tasks:
          - name: Create user summary
            type: community-llm.LlmAgent
            prompt: Create a nice summary for the currently logged in user.
            model: Gemini 2.5 Flash
            mcpServer1: GitHub MCP Server
            result: ${summary}
          - name: Verify
            type: xlrelease.GateTask
            description: |-
              ## User details
              
              ${summary}
            owner: admin
        color: "#3d6c9e"
      - phase: Check phases
        tasks:
          - name: Analyze templates
            type: community-llm.LlmAgent
            prompt: Analyze all templates in the "AI Demo" folder and report on tasks that
              are duplicated across templates.
            model: Gemini 2.5 Flash
            mcpServer1: Release MCP Server
            result: ${duplicates}
          - name: Verify
            type: xlrelease.GateTask
            description: |-
              ## Template summary
              
              ${duplicates}
        color: "#3d6c9e"
      - phase: Multiple MCP servers
        tasks:
          - name: Create GitHub ticket for last failed release
            type: community-llm.LlmAgent
            description: |-
              Attempt to make the agent "create a ticket for the last release that failed"
              
              ⚠️ Not working yet. The agent / models seem to get very confused and tend to give up.
            prompt: |-
              List all releases in Digital.ai Release.
              
              For the last release in FAILED state, create a GitHub issue in xebialabs-community/community-release-llm-integration repo to fix it.
              
              Mention Release title, date and cause in the ticket
            model: Gemini 2.5 Pro
            mcpServer1: Release MCP Server
            mcpServer2: GitHub MCP Server
          - name: Verify
            type: xlrelease.GateTask
            description: |-
              Check if there is a new issue created here:
              
              * https://github.com/xebialabs-community/community-release-llm-integration/issues
            owner: admin
        color: "#3d6c9e"
    variables:
      - type: xlrelease.StringVariable
        key: summary
        requiresValue: false
        showOnReleaseStart: false
        label: Result
        description: The result returned by the agent
      - type: xlrelease.StringVariable
        key: duplicates
        requiresValue: false
        showOnReleaseStart: false
        label: Result
        description: The result returned by the agent
  - template: 5.  Interactive Chat example
    phases:
      - phase: Prompt
        tasks:
          - name: Chat with agent
            type: community-llm.LlmChat
            model: Amazon Nova Micro 1:0
          - name: OK?
            type: xlrelease.GateTask
            owner: admin
    scriptUsername: admin
    scriptUserPassword: admin

  - template: Compare LLM Summaries
    defaultTargetFolder: AI Demo
    phases:
      - phase: Compare and judge
        tasks:
          - name: Provide a text to summarize
            type: xlrelease.UserInputTask
            description: |-
              The source text will be summarized by three different AI models: GPT-5, Smol (a very small local model) and Llama 3.2
              
              Finally, the Gemini model will compare and judge the results.
            variables:
              - text
          - name: Summarize
            type: xlrelease.ParallelGroup
            tasks:
              - name: OpenAI
                type: community-llm.LlmPrompt
                variableMapping:
                  response: ${summary1}
                capabilities:
                  - remote
                prompt: |-
                  ${summaryPrompt}
                  
                  Text:
                  
                  ${text}
                model: OpenAI GPT 5
              - name: Smol
                type: community-llm.LlmPrompt
                variableMapping:
                  response: ${summary2}
                capabilities:
                  - remote
                prompt: |-
                  ${summaryPrompt}
                  
                  Text:
                  
                  ${text}
                model: SmolLM2 (local)
              - name: Llama
                type: community-llm.LlmPrompt
                variableMapping:
                  response: ${summary3}
                capabilities:
                  - remote
                prompt: |-
                  ${summaryPrompt}
                  
                  Text:
                  
                  ${text}
                model: Llama 3.2 (local)
          - name: Resulting summaries
            type: xlrelease.Task
            description: |-
              These are the results that the LLM's came up with:
              
              ## GPT -5
              
              ${summary1}
              
              ---
              ## Smol
              
              ${summary2}
              
              ---
              ## Llama 3.2
              
              ${summary3}
          - name: Compare (Gemini)
            type: community-llm.LlmPrompt
            description: Comparing the results and creating a jury report.
            variableMapping:
              response: ${report}
            capabilities:
              - remote
            prompt: |-
              Please compare the following summaries made by AI models and judge which one was best. Provide a ranking.
              
              ---
              Original text:
              
              ${text}
              
              ---
              1. ChatGPT:
              
              ${summary1}
              
              ---
              2. Smol (local):
              
              ${summary2}
              
              ---
              3. Llama 3.2 (local):
              
              ${summary3}
            model: Gemini 2.5 Flash
          - name: Jury report
            type: xlrelease.Task
            description: "${report}"
    kind: WORKFLOW
    variables:
      - type: xlrelease.StringVariable
        key: summaryPrompt
        label: Summary prompt
        description: Prompt to ask for the summary
        value: |-
          Please summarize the following text in a concise way of maximum 100 words.
          Avoid duplication, meaningless filler words and adjectives that only add tone
          but not meaning.
        multiline: true
      - type: xlrelease.StringVariable
        key: text
        requiresValue: false
        showOnReleaseStart: false
        label: Source text
        description: The text to summarize
        multiline: true
      - type: xlrelease.StringVariable
        key: summary1
        requiresValue: false
        showOnReleaseStart: false
        description: The AI response
      - type: xlrelease.StringVariable
        key: summary2
        requiresValue: false
        showOnReleaseStart: false
        description: The AI response
      - type: xlrelease.StringVariable
        key: summary3
        requiresValue: false
        showOnReleaseStart: false
        description: The AI response
      - type: xlrelease.StringVariable
        key: report
        requiresValue: false
        showOnReleaseStart: false
        description: The AI response

  - template: Tasks for README screenshots
    variables:
      - type: xlrelease.StringVariable
        key: issue
        requiresValue: false
        showOnReleaseStart: false
        label: Issue
        description: The issue json from GitHub
    scriptUsername: admin
    scriptUserPassword: admin
    phases:
      - phase: Examples
        color: "#3d6c9e"
        tasks:
          - name: Report failed releases
            type: community-llm.LlmAgent
            description: Use an agent to automate your workflow
            prompt: |-
              Create a ticket for each failed release.
              
              Use the issue list in xebialabs-community/community-release-llm-integration
            model: Anthropic Sonnet 4.5
            mcpServer1: Release MCP Server
            mcpServer2: GitHub MCP Server

          - name: Say hello
            type: community-llm.LlmPrompt
            prompt: Say hello in 6 languages for 6 continents
            model: Amazon Nova Micro 1:0

          - name: Interactive chat
            type: community-llm.LlmChat
            model: Gemini 2.5 Flash

          - name: Read GitHub issue
            type: community-llm.McpCallTool
            description: Read issue number `1` from GitHub repo **community-release-llm-integration**
            result: ${issue}
            server: GitHub MCP Server
            tool: issue_read
            input: |-
              {
              "issue_number": 1,
              "method": "get",
              "owner": "xebialabs-community",
              "repo": "community-release-llm-integration"
              }

          - name: List Release MCP tools
            type: community-llm.McpListToolsTask
            description: Lists all MCP tools for this Release server
            server: Release MCP Server

